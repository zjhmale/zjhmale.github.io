---
layout: post
title: "cljmlchapter3-category data"
date: 2014-08-08 09:44:20 +0800
comments: true
categories: clojure ml
---

举例来说，让我们假设现在需要利用一个分类器模型来对鱼类包装厂中的鱼进行分类。在这种情况下，鱼最终会被分到两个独立的类别中。这里假设我们最终将鱼最终分到鲈鱼或者是三文鱼类中。我们需要选取足够多样本数据作为训练数据用于训练我们的模型，并且还需要分析这些数据在一些选中的特征上的分布情况。这里我们使用两个特征来分类数据，分别是鱼的长度和表皮的亮度。

鱼的长度这个特征值得分布可以如下图所示：

<center>
	<img src="/images/cljml/chap3/image1.png">
</center>

同样的，我们也可以可视化出鱼的表皮亮度在样本数据中的分布情况，如下图所示：

<center>
	<img src="/images/cljml/chap3/image2.png">
</center>

从上面两个分布图来看，我们看到如果仅仅只有鱼的长度这一个特征是没有办法获取足够的信息去对鱼做分类操作的。因此鱼的长度这一个特征在分类模型中的系数会相对较小。相反的，因为鱼表皮的亮度这一个特征在决定鱼的类型时扮演着更重要的角色，所以这个特征在最终的预估分类模型中权值系数向量中对应的系数值会更大一些。

一旦我们对已有的分类问题进行了建模，我们就可以将训练数据划分成两个(或者更多)个类别集中。这个在定制分类模型中用来在向量空间中将数据进行类别划分的超平面也叫做**决策边界(decision boundary)**。在决策边界一侧的所有点都属于某一个类，而在决策边界另一侧的所有点则属于是另一个类别。一个很明显的推论就是，根据需要区分的独立类别的个数，一个给定的分类器模型可以有好几个这样的决策平面。

现在我们可以整合这两个特征来训练我们的模型了，并最终会产生一个预估决策边界来划分鱼的两个类别。可以用如下的散点图来可视化这个决策边界作用于训练数据之上的效果：

<center>
	<img src="/images/cljml/chap3/image3.png">
</center>

如上图所示，我们近似地使用一个直线去作为分类器模型的决策边界，因此，我们是将这个分类模型当做了一个线性函数。当然我们也可以让这个分类器以高阶多项式函数的形式去对样本数据进行建模，使用高阶多项式也许可以得到一个精度更高的分类器模型。可以用下图来可视化此时分类器的决策边界：

<center>
	<img src="/images/cljml/chap3/image4.png">
</center>

上图所示的用来划分数据的决策边界都是基于二维特征的。当训练数据具有更高维度的特征的时候，决策边界将会变得很复杂以至于在二维空间中很难可视化出来。例如有三个特征，那么决策边界将会是一个三维空间中的一个平面，如下图所示。需要注意的是，为了清楚起见，样本数据点并没有在下图中标出。从下图也可以看出，样本数据中其中两个维度的变化范围在$$[-10, 10]$$内，第三个特征的数值变化范围在$$[-200, 200]$$内。

<center>
	<img src="/images/cljml/chap3/image5.png">
</center>

## 理解贝叶斯分类

现在我们将会探索贝叶斯分类技术从而分类数据。一个**贝叶斯分类器**本质上是一个基于贝叶斯理论的概率分类器，贝叶斯理论是基于条件概率。一个基于贝叶斯分类器的模型会假设样本数据中的每一个特征都是完全独立的。对于独立，意味着模型中的每一个特征都可以独立于其他的特征单独变化。换句话说，模型中的特征是相互排斥的。因此，一个贝叶斯分类器会假设分类模型中某一个特定的特征存在与否和模型中其他的特征存在与否完全独立，互不影响。

$$P(A)$$这一项被用来表示特征A出现的概率。这个值是一个在$$[0, 1]$$范围内的概率值。当然也可以用百分数来表示这个值。例如，$$0.5$$这个概率值也可以被写作$$50\%$$或者$$50 percent$$。假设我们现在想要找到一个特征A在一个给定的样本集中出现的概率。因此$$P(A)$$的值越大说明特征A有更高的机会出现。我们可以用如下的公式形式化的表示$$P(A)$$这一项：

$$P(A) = \frac{样本集中有特征A的样本数量}{样本集中总的样本数量}$$


