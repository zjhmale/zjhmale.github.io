---
layout: post
title: "cljmlchapter6-svm"
date: 2014-08-18 09:44:20 +0800
comments: true
categories: clojure ml
---

在这一章中，我们将会探索**支持向量机(SVMs)**。我们将会学习多种用Clojure实现支持向量机的方法与原理，并最后利用给定的训练数据建立和训练一个支持向量机。

支持向量机是一种既可以用于解决分类问题也可以用于解决回归问题的监督学习模型。但是在这一章中，我们会把重点放在用支持向量机解决分类问题。支持向量机可以被用于文本挖掘，化学分类以及图像识别和手写字识别。当然，我们需要了解到的事实是，一个机器学习模型性能的好坏很大程度上取决于训练数据的性质以及我们如何调整和优化机器学习模型。

简单的来说，支持向量机利用向量空间中两个类别之间的一个评估得到的最优超平面来分类和预测两个类别的数据。一个超平面比外部的向量空间少一维，比如在一个三维空间中，我们会得到一个二维的超平面。

一个最基本的支持向量机就是一个用于线性分类的非概率模型的二元分类器。但是支持向量机不仅可以解决线性分类问题，还可以用于非线性问题和多分类问题。关于支持向量机一个有趣的事实是支持向量机会在不同类别样本数据之间利用间隔最大化得到一个唯一的分割超平面来划分不同的类别(所谓间隔最大化是指离分割超平面最近的点到分割超平面的距离最大化)。也正是基于这个有趣的特性，支持向量机总是拥有很好的泛化性能并且也实现了自动控制模型复杂度的机制来防止过拟合的发生。因此支持向量机也称作**最大间隔分类器**。在这一章中我们也会研究相对于其他分类器，支持向量机是如何找到这个最大间隔的。关于支持向量机另一个有趣的事实是，支持向量机可以很好地支持模型特征的扩展，所以支持向量机也经常应用在有大量特征的机器学习问题上。

## 理解最大间隔分类

如我们之前提到的，支持向量机利用间隔最大化来进行分类，让我们来看看这是如何做到的。首先我们需要引入在第三章中使用的逻辑斯底分类模型作为支持向量机的基础。

在第三章中我们使用逻辑斯底函数或者说是*sigmoid*函数来对两个不同的类别的输入进行分类。这个函数可以用如下的公式形式化地表示出来，其中$$X$$表示的是输入的样本数据：

$$Y = \frac{1}{1+e^{- \beta ^{T} \cdot X}}$$

从上面的等式中可以看出，因变量$$Y$$不仅和自变量$$X$$有关，还和参数$$\beta$$有关。其中自变量$$X$$表示的是模型的输入值向量，$$\beta$$表示的是模型中与对应不同特征的权值向量。对于二元分类来说，因变量$$Y$$的值必须在[0, 1]的范围内。最终分类的结果也是看因变量$$Y$$的值是更接近于0还是更接近于1。因此$$\beta^{T} \cdot X$$这一项的值如果不是远大于0，就是远小于0。这种关系可以形式化地表达：

$$\begin{align*}
if & Y \approx 1, \; \beta^{T} \cdot X \gg 0 \\
& Y \approx 0, \; \beta^{T} \cdot X \ll 0
\end{align*}$$

如果有N个训练样本，每一个训练样本的输入用$$X_{i}$$表示，输出用$$Y_{i}$$表示，我们可以定义损失函数，如下所示：

$$J(\beta) = \frac{-1}{N} \sum_{i=0}^{N}\left( Y_{i}log\hat{Y_{i}} + (1-Y_{i})log(1-\hat{Y_{i}}) \right)$$

>注意$$\hat{Y_{i}}$$这一项代表了预估模型利用样本输入值计算出来的实际输出值A

对于一个逻辑斯蒂回归模型来说，$$\hat{Y_{i}}$$这一项表示将样本输入$$X_{i}$$传入逻辑斯底函数得到的实际输出值。我们可以继续展开上面等式定义的损失函数中的求和项$$Y_{i}log\hat{Y_{i}} + (1-Y_{i})log(1-\hat{Y_{i}})$$，如下所示：

$$\begin{align*}
& - \left(YlogY + (1-\hat{Y})log(1-\hat{Y}) \right) \\
& = -Ylog \left(\frac{1}{1+e^{-\beta^{T}X}} \right) - (1-Y)log \left(1-\frac{1}{1+e^{-\beta^{T}X}} \right)
\end{align*}$$

很容易就可以发现损失函数的值取决于上面表达式中所示的两个对数项的值。因此，我们可以将损失函数表示成关于这两个对数项的函数，这两个对数项也分别使用$$cost_{0}$$和$$cost_{1}$$表示，如下所示：

$$Let \; cost_{0} = -log\left(\frac{1}{1+e^{-\beta^{T}X}} \right) \; and \; cost_{1} = -log\left(1-\frac{1}{1+e^{-\beta^{T}X}} \right)$$

$$cost_{0}$$函数与$$cost_{1}$$函数都是用逻辑斯蒂函数组成的。一个使用逻辑斯蒂函数进行建模的分类器必须不断训练，使得对于传入的权值向量$$\beta$$，$$cost_{0}$$和$$cost_{1}$$两个函数都能保证最小。在线性分类中我们可以使用**hinge-loss**函数来达到和逻辑斯底函数一样的期望行为(更多信息可以参考"Are Loss Functions All the Same?"这篇论文)。我们现在来通过和逻辑斯蒂函数进行对比的方式来研究hinge-loss函数。下面的图标中描述了$$cost_{0}$$函数分别为逻辑斯底函数和hinge-loss函数时的函数值随着$$\beta^{T}X$$这一项的值变化而变化的趋势：

<center>
	<img src="/images/cljml/chap6/image1.png">
</center>

在上面所示的图表中，光滑曲线代表的是$$cost_{0}$$函数是逻辑斯蒂函数时的变化趋势，这个函数在某一个给定点之前迅速下降，在给定点之后下降的速率变小了。在当前例子中，$$cost_{0}$$函数下降速率发生转变的给定点是$$x=0$$。使用hinge-loss函数可以接近这种行为，只不过此时变化趋势变成了两条线段，两条线段的连接点依然是在$$x=0$$处。有趣的是，不管是使用逻辑斯蒂函数还是hinge-loss函数，$$cost_{0}$$函数都是相对输入值x以负相关的趋势变化，并且都在同一个给定点上发生了下降速率的变化。同样的，我们还可以画出$$cost_{1}$$函数分别以两种形式表示时的变化趋势：

<center>
	<img src="/images/cljml/chap6/image2.png">
</center>

需要注意的是，$$cost_{1}$$函数在两种形式下相对于$$\beta^{T}X$$这一项是正相关趋势变化。因此我们使用hinge-loss函数来构造损失函数可以获得和使用逻辑斯蒂函数一样的分类能力。并且一个使用hinge-loss函数来构建的分类器将会拥有和逻辑斯蒂函数构建的分类器差不多的性能。

可以从上面的趋势变化图表中看出，不管是在$$cost_{0}$$函数还是在$$cost_{1}$$函数中，hinge-loss函数的变化速率只有在$$\beta^{T}X=0$$处才会变化。因此我们利用hinge-loss函数，根据$$\beta^{T}X$$这一项的值是大于0还是小于0来对两个不同类别的样本进行分类。在这个情况下，其实两个类之间的样本点之间就没有分类间隔了。此时的情况适用公式描述如下：

$$\begin{align*}
& \underset{\beta}{max} \; \frac{\hat{\gamma}}{\left \| \beta \right \|} \\
& s.t. \; y_{i} \cdot (\beta^{T}X) \geq 0
\end{align*}$$

此时虽然要找一个最大间隔$$\hat{\gamma}$$但是，但是不同类别样本到分割平面的距离只是限制要大于0，也就是说离分割超平面最近的点到超平面的距离可以为0。这就造成了分类样本点之间会没有分类间隔，而最大间隔分类指的是不仅要将正负示例分开，而且对最难分的实例点也就是离分割超平面最近的点也要有足够大的区分度将它们分到正确的类别中。所以为了提升分类的间隔，我们可以修改hinge-loss函数使得只有当$$\beta^{T}X \geq -1$$或者$$\beta^{T}X \leq 1$$时hinge-loss函数的值才大于0。此时的情况可以用公式描述如下：

$$\begin{align*}
& \underset{\beta}{max} \; \frac{\hat{\gamma}}{\left \| \beta \right \|} \\
& s.t. \; y_{i} \cdot (\beta^{T}X) \geq \hat{\gamma}
\end{align*}$$

由于函数间隔对上面的最优化问题的结果并不影响，对目标函数的优化也不影响所以此时可以将$$\hat{\gamma}=1$$代入，并且最大化$$\frac{1}{\left \| \beta \right \|}$$和最小化$$\frac{1}{2} \cdot \left \| \beta \right \|^{2}$$等价，所以可以最终得到要求解的最优化问题如下：

$$\begin{align*}
& \underset{\beta}{min} \; \frac{1}{2} \cdot \left \| \beta \right \|^{2} \\
& s.t. \; y_{i} \cdot (\beta^{T}X) - 1 \geq 0
\end{align*}$$

由于$$y_{i}$$的取值只有-1和1两种，所以我们就可以得出之前要修改hinge-loss函数使之满足的条件了，也就是只有在有样本点距离分割平面的距离小于最大间隔时，才会增加损失函数的值，说明还需要继续训练。

修改后的hinge-loss函数的变化趋势可以用下面的图表表示，仍然由于$$y_{i}$$的取值只有-1和1两种，所以两种不同类别的样本实际上只会使用到$$cost_{0}$$函数或者$$cose_{1}$$函数，不会两个函数同时用到。下图所示的是hinge-loss函数对$$\beta^{T}X \geq 1$$这一类样本的变化趋势图表：

<center>
	<img src="/images/cljml/chap6/image3.png">
</center>

同样的，修改之后的hinge-loss函数对于$$\beta^{T}X \leq -1$$这一类的样本的变化趋势如下图所示：

<center>
	<img src="/images/cljml/chap6/image4.png">
</center>

需要注意的是上图中hinge-loss函数的转折点在$$x=-1$$处。

一个很清晰的结论就是，只有在$$\beta^{T}X \leq -1$$或者$$\beta^{T}X \geq 1$$时，也就是样本点距离分割超平面的距离都至少为最大间隔，此时训练的损失值都是0，损失函数不会增大，而在$$\beta^{T}X \geq -1$$或者$$\beta^{T}X \leq 1$$时损失值才为正值，此时会增大损失函数的值，说明还需要继续训练我们的支持向量机，从而使得最终的损失函数可以尽可能的小甚至接近于0。

