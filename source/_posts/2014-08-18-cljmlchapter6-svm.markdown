---
layout: post
title: "cljmlchapter6-svm"
date: 2014-08-18 09:44:20 +0800
comments: true
categories: clojure ml
---

在这一章中，我们将会探索**支持向量机(SVMs)**。我们将会学习多种用Clojure实现支持向量机的方法与原理，并最后利用给定的训练数据建立和训练一个支持向量机。

支持向量机是一种既可以用于解决分类问题也可以用于解决回归问题的监督学习模型。但是在这一章中，我们会把重点放在用支持向量机解决分类问题。支持向量机可以被用于文本挖掘，化学分类以及图像识别和手写字识别。当然，我们需要了解到的事实是，一个机器学习模型性能的好坏很大程度上取决于训练数据的性质以及我们如何调整和优化机器学习模型。

简单的来说，支持向量机利用向量空间中两个类别之间的一个评估得到的最优超平面来分类和预测两个类别的数据。一个超平面比外部的向量空间少一维，比如在一个三维空间中，我们会得到一个二维的超平面。

一个最基本的支持向量机就是一个用于线性分类的非概率模型的二元分类器。但是支持向量机不仅可以解决线性分类问题，还可以用于非线性问题和多分类问题。关于支持向量机一个有趣的事实是支持向量机会在不同类别样本数据之间利用间隔最大化得到一个唯一的分割超平面来划分不同的类别(所谓间隔最大化是指离分割超平面最近的点到分割超平面的距离最大化)。也正是基于这个有趣的特性，支持向量机总是拥有很好的泛化性能并且也实现了自动控制模型复杂度的机制来防止过拟合的发生。因此支持向量机也称作**最大间隔分类器**。在这一章中我们也会研究相对于其他分类器，支持向量机是如何找到这个最大间隔的。关于支持向量机另一个有趣的事实是，支持向量机可以很好地支持模型特征的扩展，所以支持向量机也经常应用在有大量特征的机器学习问题上。

## 理解最大间隔分类

如我们之前提到的，支持向量机利用间隔最大化来进行分类，让我们来看看这是如何做到的。首先我们需要引入在第三章中使用的逻辑斯底分类模型作为支持向量机的基础。

在第三章中我们使用逻辑斯底函数或者说是*sigmoid*函数来对两个不同的类别的输入进行分类。这个函数可以用如下的公式形式化地表示出来，其中$$X$$表示的是输入的样本数据：

$$Y = \frac{1}{1+e^{- \beta ^{T} \cdot X}}$$

从上面的等式中可以看出，因变量$$Y$$不仅和自变量$$X$$有关，还和参数$$\beta$$有关。其中自变量$$X$$表示的是模型的输入值向量，$$\beta$$表示的是模型中与对应不同特征的权值向量。对于二元分类来说，因变量$$Y$$的值必须在[0, 1]的范围内。最终分类的结果也是看因变量$$Y$$的值是更接近于0还是更接近于1。因此$$\beta^{T} \cdot X$$这一项的值如果不是远大于0，就是远小于0。这种关系可以形式化地表达：

$$\begin{align*}
if & Y \approx 1, \; \beta^{T} \cdot X \gg 0 \\
& Y \approx 0, \; \beta^{T} \cdot X \ll 0
\end{align*}$$

如果有N个训练样本，每一个训练样本的输入用$$X_{i}$$表示，输出用$$Y_{i}$$表示，我们可以定义损失函数，如下所示：

$$J(\beta) = \frac{-1}{N} \sum_{i=0}^{N}\left( Y_{i}log\hat{Y_{i}} + (1-Y_{i})log(1-\hat{Y_{i}}) \right)$$

>注意$$\hat{Y_{i}}$$这一项代表了预估模型利用样本输入值计算出来的实际输出值A

对于一个逻辑斯蒂回归模型来说，$$\hat{Y_{i}}$$这一项表示将样本输入$$X_{i}$$传入逻辑斯底函数得到的实际输出值。我们可以继续展开上面等式定义的损失函数中的求和项$$Y_{i}log\hat{Y_{i}} + (1-Y_{i})log(1-\hat{Y_{i}})$$，如下所示：

$$\begin{align*}
& - \left(YlogY + (1-\hat{Y})log(1-\hat{Y}) \right) \\
& = -Ylog \left(\frac{1}{1+e^{-\beta^{T}X}} \right) - (1-Y)log \left(1-\frac{1}{1+e^{-\beta^{T}X}} \right)
\end{align*}$$

很容易就可以发现损失函数的值取决于上面表达式中所示的两个对数项的值。因此，我们可以将损失函数表示成关于这两个对数项的函数，这两个对数项也分别使用$$cost_{0}$$和$$cost_{1}$$表示，如下所示：

$$Let \; cost_{0} = -log\left(\frac{1}{1+e^{-\beta^{T}X}} \right) \; and \; cost_{1} = -log\left(1-\frac{1}{1+e^{-\beta^{T}X}} \right)$$

$$cost_{0}$$函数与$$cost_{1}$$函数都是用逻辑斯蒂函数组成的。一个使用逻辑斯蒂函数进行建模的分类器必须不断训练，使得对于传入的权值向量$$\beta$$，$$cost_{0}$$和$$cost_{1}$$两个函数都能保证最小。在线性分类中我们可以使用**hinge-loss**函数来达到和逻辑斯底函数一样的期望行为(更多信息可以参考"Are Loss Functions All the Same?"这篇论文)。我们现在来通过和逻辑斯蒂函数进行对比的方式来研究hinge-loss函数。下面的图标中描述了$$cost_{0}$$函数分别为逻辑斯底函数和hinge-loss函数时的函数值随着$$\beta^{T}X$$这一项的值变化而变化的趋势：

<center>
	<img src="/images/cljml/chap6/image1.png">
</center>

在上面所示的图表中，光滑曲线代表的是$$cost_{0}$$函数是逻辑斯蒂函数时的变化趋势，这个函数在某一个给定点之前迅速下降，在给定点之后下降的速率变小了。在当前例子中，$$cost_{0}$$函数下降速率发生转变的给定点是$$x=0$$。使用hinge-loss函数可以接近这种行为，只不过此时变化趋势变成了两条线段，两条线段的连接点依然是在$$x=0$$处。有趣的是，不管是使用逻辑斯蒂函数还是hinge-loss函数，$$cost_{0}$$函数都是相对输入值x以负相关的趋势变化，并且都在同一个给定点上发生了下降速率的变化。同样的，我们还可以画出$$cost_{1}$$函数分别以两种形式表示时的变化趋势：

<center>
	<img src="/images/cljml/chap6/image2.png">
</center>

需要注意的是，$$cost_{1}$$函数在两种形式下相对于$$\beta^{T}X$$这一项是正相关趋势变化。因此我们使用hinge-loss函数来构造损失函数可以获得和使用逻辑斯蒂函数一样的分类能力。并且一个使用hinge-loss函数来构建的分类器将会拥有和逻辑斯蒂函数构建的分类器差不多的性能。

可以从上面的趋势变化图表中看出，不管是在$$cost_{0}$$函数还是在$$cost_{1}$$函数中，hinge-loss函数的变化速率只有在$$\beta^{T}X=0$$处才会变化。因此我们利用hinge-loss函数，根据$$\beta^{T}X$$这一项的值是大于0还是小于0来对两个不同类别的样本进行分类。在这个情况下，其实两个类之间的样本点之间就没有分类间隔了。此时的情况适用公式描述如下：

$$\begin{align*}
& \underset{\beta}{max} \; \frac{\hat{\gamma}}{\left \| \beta \right \|} \\
& s.t. \; y_{i} \cdot (\beta^{T}X) \geq 0
\end{align*}$$

此时虽然要找一个最大间隔$$\hat{\gamma}$$但是，但是不同类别样本到分割平面的距离只是限制要大于0，也就是说离分割超平面最近的点到超平面的距离可以为0。这就造成了分类样本点之间会没有分类间隔，而最大间隔分类指的是不仅要将正负示例分开，而且对最难分的实例点也就是离分割超平面最近的点也要有足够大的区分度将它们分到正确的类别中。所以为了提升分类的间隔，我们可以修改hinge-loss函数使得只有当$$\beta^{T}X \geq -1$$或者$$\beta^{T}X \leq 1$$时hinge-loss函数的值才大于0。此时的情况可以用公式描述如下：

$$\begin{align*}
& \underset{\beta}{max} \; \frac{\hat{\gamma}}{\left \| \beta \right \|} \\
& s.t. \; y_{i} \cdot (\beta^{T}X) \geq \hat{\gamma}
\end{align*}$$

由于函数间隔对上面的最优化问题的结果并不影响，对目标函数的优化也不影响所以此时可以将$$\hat{\gamma}=1$$代入，并且最大化$$\frac{1}{\left \| \beta \right \|}$$和最小化$$\frac{1}{2} \cdot \left \| \beta \right \|^{2}$$等价，所以可以最终得到要求解的最优化问题如下：

$$\begin{align*}
& \underset{\beta}{min} \; \frac{1}{2} \cdot \left \| \beta \right \|^{2} \\
& s.t. \; y_{i} \cdot (\beta^{T}X) - 1 \geq 0
\end{align*}$$

由于$$y_{i}$$的取值只有-1和1两种，所以我们就可以得出之前要修改hinge-loss函数使之满足的条件了，也就是只有在有样本点距离分割平面的距离小于最大间隔时，才会增加损失函数的值，说明还需要继续训练。

修改后的hinge-loss函数的变化趋势可以用下面的图表表示，仍然由于$$y_{i}$$的取值只有-1和1两种，所以两种不同类别的样本实际上只会使用到$$cost_{0}$$函数或者$$cose_{1}$$函数，不会两个函数同时用到。下图所示的是hinge-loss函数对$$\beta^{T}X \geq 1$$这一类样本的变化趋势图表：

<center>
	<img src="/images/cljml/chap6/image3.png">
</center>

同样的，修改之后的hinge-loss函数对于$$\beta^{T}X \leq -1$$这一类的样本的变化趋势如下图所示：

<center>
	<img src="/images/cljml/chap6/image4.png">
</center>

需要注意的是上图中hinge-loss函数的转折点在$$x=-1$$处。

一个很清晰的结论就是，只有在$$\beta^{T}X \leq -1$$或者$$\beta^{T}X \geq 1$$时，也就是样本点距离分割超平面的距离都至少为最大间隔，此时训练的损失值都是0，损失函数不会增大，而在$$\beta^{T}X \geq -1$$或者$$\beta^{T}X \leq 1$$时损失值才为正值，此时会增大损失函数的值，说明还需要继续训练我们的支持向量机，从而使得最终的损失函数可以尽可能的小甚至接近于0。

假如我们使用hinge-loss函数来取代$$cost_{0}$$函数和$$cost_{1}$$函数。那我们就要面对一个支持向量机中的优化问题(更多信息可以参考"Support-vector networks"这篇论文)。这个优化问题可以使用下面公式形式化描述：

$$\underset{\beta}{arg \; min}\left [ \frac{1}{N} \sum_{i=1}^{N}Y_{i}cost_{0}(\beta^{T}X_{i}) + (1-Y_{i})cost_{1}(\beta^{T}X_{i}) + \frac{\lambda}{2N}\sum_{j=0}^{N}[\beta_{j}^{2}] \right ]$$

在上面的等式中，$$\lambda$$这一项是正则化项的正则化系数。当$$Y_{i} \approx 1$$，那么支持向量机的行为更多地会受到$$cost_{0}$$函数的影响，相反的当$$Y_{i} \approx 0$$时，支持向量机的行为则更多的会受到$$cost_{1}$$函数的影响。在某些情况下，正则化系数$$\lambda$$可以以常数C的形式加入到这个最优化问题中，其中常数C为$$\frac{1}{\lambda}$$。此时表达最优化问题的公式变为如下形式：

$$\underset{\beta}{arg \; min}\left [ C\sum_{i=1}^{N}Y_{i}cost_{0}(\beta^{T}X_{i}) + (1-Y_{i})cost_{1}(\beta^{T}X_{i}) + \frac{1}{2}\sum_{j=0}^{N}[\beta_{j}^{2}] \right ]$$

由于我们只是面对一个二分类问题，$$Y_{i}$$的取值为0或者1，我们可以重写上面的最优化问题使之表述起来更为精简优雅，如下所示：

$$\begin{align*}
& \underset{\beta}{min} \frac{1}{2} \sum_{i} \beta_{i}^{2} \; s.t. \\
& \beta^{T}X_{i} \geq 1 \; if \; Y_{i} = 1 \\
& \beta^{T}X_{i} \leq -1 \; if \; Y_{i} = 0
\end{align*}$$

让我们试着可视化出支持向量机在一些训练数据上的行为。假设我们的数据用两个特征维度$$x_{1}$$与$$x_{2}$$。这些训练数据点可以用下面的图标描述：

<center>
	<img src="/images/cljml/chap6/image5.png">
</center>

在上面的图表中，分别使用方块和圆圈来表示两个类别的样本数据点。一个线性分类器会试图用上图中所画的许多直线中的一条直线作为决策边界来将图中的样本点分类到两个独立的类别中。当然这个定制的模型不仅需要尽可能地减小总误差，而且还不能让模型对训练数据过拟合要保证有较好的泛化能力。和其他的分类模型一样，支持向量机也会尽可能地将样本数据分类到两个类别中。然后不同的是，支持向量机会在两类输入样本点中确定一个分割超平面，而两个类别中离最终确定的分割超平面的距离将会尽量最大化。这样对于最难以区分的点也有很强的分类能力，可以保证支持向量机的泛化性能。支持向量机的这种行为可以用下面的图标形象的表示出来：

<center>
	<img src="/images/cljml/chap6/image6.png">
</center>

如上图所示，支持向量机会使用两个类之间的最大间隔来确定一个合适的超平面来对两个类别的数据样本进行分类。对于之前提到的这个最优的分割超平面，我们可以用一个等式来形式化的描述，如下所示：

$$\left( \beta^{T}X + \beta_{0} \right) = 0$$

>注意在上面的等式中，$$\beta_{0}$$这一个常数表示的是分割超平面Y轴的截距值。

为了更深入地理解支持向量机是如何获得最大间隔分割平面的，我们需要使用一些基本的向量运算。首先，我们来定义一个给定向量的模长：

$$假如U是一个由元素[u_{i}]组成的向量$$

$$\left \| U \right \| = \sqrt{\sum_{i}u_{i}}，\; 为向量U的模长$$

在描述支持向量机时还有一种常用的向量运算操作是计算两个向量的内积。计算两个向量的内积可以形式化地表述如下：

$$对于两个具有相同元素个数的向量U=[u_{i}]和V=[v_{i}]$$

$$\left \langle U,V \right \rangle = U \cdot V = \sum_{i=0}^{n}u_{i}v_{i} \; 表示向量U与向量V的内积$$

>需要注意的是，只有当两个向量含有的元素个数相同时才能计算这两个向量的内积。

从上面的等式中可以看到，向量U和向量V的内积$$\left \langle U,V \right \rangle$$的值和向量U的转置与向量V的点积$$U^{T}V$$的值是相同的。此外我们还可以通过一个向量投影到另一个向量的角度来理解两个向量的内积：

$$\left \langle U,V \right \rangle = U \cdot V = P \cdot \left \| U \right \|，\; 其中P是向量V在向量U方向上投影的长度$$

需要注意到$$U \cdot V$$这一项等于向量U的转置与向量V的点积$$U^{T}V$$相同。并且$$P \cdot \left \| U \right \|$$与向量的点积$$U^{T}V$$相同，我们可以从输入值向量在权值向量方向上投影的角度来重新描述之前提到的最优化问题，可以形式化地表达如下：

$$\begin{align*}
& \underset{\beta}{arg \; min} \frac{1}{2} \sum_{i} \beta_{i}^{2} \; s.t. \\
& 假如 \; Y_{i} = 1, \; P_{i} \cdot \left \| \beta \right \| \geq 1 \\
& 假如 \; Y_{i} = 0, \; P_{i} \cdot \left \| \beta \right \| \leq -1 \\
\end{align*}$$

$$其中P_{i}表示X_{i}在\beta方向上投影的长度$$

因此，训练支持向量机的过程是尽可能最小化权值向量$$\beta$$中所有元素的平方和，并且保证用来对两个类别分类的最优分割平面在$$P_{i} \cdot \left \| \beta \right \| = 1$$和$$P_{i} \cdot \left \| \beta \right \| = -1$$两个平面之间。这两个平面称为支持向量机的**支持向量**。因为我们需要尽可能地减小权值向量$$\beta$$的模长，所以投影$$P_{i}$$的大小要尽可能的大，从而保证满足

$$\begin{align*}
& 假如 \; Y_{i} = 1, \; P_{i} \cdot \left \| \beta \right \| \geq 1 \\
& 假如 \; Y_{i} = 0, \; P_{i} \cdot \left \| \beta \right \| \leq -1
\end{align*}$$

这两个条件，从而又如下推论：

$$如果\beta \ll 0，那么必须有P_{i} \gg 0$$

因此，支持向量机需要保证输入向量$$X_{i}$$在权值向量$$\beta$$上的投影尽可能大。这使得支持向量机可以找到训练数据集中两个不同类别样本点之间的最大间隔。

## 支持向量机的其他形式

现在我们将会接触支持向量机的一些其他形式。如果对这些形式不感兴趣，完全可以跳过本节的内容，完全不会影响后面内容的阅读。但是还是建议读者了解一下这些形式，因为这些形式在支持向量机理论中也是被广泛使用的。

假如$$w$$垂直于支持向量机评估出来的分类超平面，也就是$$w$$在超平面的法向方向上，那么我们可以使用如下所示的等式来形式化地描述这个分类用的超平面：

$$w \cdot X - b = 0$$

>需要注意的是，在上面的等式中，$$b$$这一项表示超平面在y轴上的截距值，和我们之前用于描述超平面的等式中的$$\beta_{0}$$这一项类似。

超平面两边的支持向量也可以用如下的等式形式化描述：

$$w \cdot X -b = 1 \; 和 \; w \cdot X -b = -1$$

现在我们可以利用$$w \cdot X - b$$这一个表达式来确定给定输入样本的类别。假如这个表达式的值小于等于-1，那么我们就可以认为输入的样本属于第一个类别，同样的假如表达式$$w \cdot X - b$$的值大于等于1，那么输入样本的类别就被预测为是属于第二个类别。这个行文可以形式化地描述，如下所示：

$$\begin{align*}
假如 & \hat{Y_{i}} \approx 1, \; w \cdot X_{i} - b \geq 1 \\
& \hat{Y_{i}} \approx -1, \; w \cdot X_{i} - b \leq -1
\end{align*}$$

上面所使用的两个不等式其实可以变换为一种更简洁的形式，只是用一个不等式来描述：

$$\hat{Y_{i}} \cdot (w \cdot X_{i} - b) \geq 1$$

因此我们可以用一种更简洁的形式来描述支持向量机要解决的最优化问题，如下所示：

$$\begin{align*}
& \underset{w, b}{arg \; min} \left [ \frac{1}{2} \left \| w \right \|^2 \right ] \\
& s.t. \; Y_{i} \cdot (w \cdot X_{i} - b) \geq 1
\end{align*}$$

在上面描述的约束最优化问题中，我们使用了$$w$$代替了权值向量$$\beta$$。如果使用拉格朗日乘子$$\alpha$$，我们就可以用如下形式描述要解决的约束最优化问题：

$$arg \; \underset{w, b}{min} \; \underset{\alpha}{max} \left [ \frac{1}{2}\left \| w \right \|^{2} - \sum_{i=1}^{N}\alpha_{i}(Y_{i}\cdot (w \cdot X_{i} - b) -1) \right ]$$

上面所示的最优化问题的形式是支持向量机要解决的最优化问题的**原始形式**。注意在实践中，只有很少一部分的拉格朗日乘子的值会大于0。此外，这个原始问题还可以表示为输入向量$$X_{i}$$和输出值$$Y_{i}$$的线性组合：

$$w = \sum_{i=1}^{N} \alpha_{i}Y_{i}X_{i}$$

因此我们可以将原来的优化问题表示成其对偶形式，同样也是一个约束优化问题，可以形式化地描述：

$$\begin{align*}
& arg \underset{\alpha}{max} \left [ \sum_{i=0}^{N}\alpha_{i} - \frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}Y_{i}Y_{j}K(X_{i}, X_{j}) \right ] \\
& s.t. \; \sum_{i=1}^{N} \alpha_{i}Y_{i} = 0 \\
& \;\;\;\;\;\; \alpha_{i} = 0, \; i = 1,2,\cdots,N
\end{align*}$$

在上述形式描述的约束优化问题中$$K(X_{i}, X_{j})$$函数也被称为**核函数(kernel function)**，我们将会在之后的章节中介绍和讨论这个函数在支持向量机中扮演的角色。

## 使用支持向量机进行先行分类

正如我们之前介绍的，支持向量机可以对两个独立的类别进行先行分类。支持向量机会试图去寻找一个超平面来分割两个类别的样本点，而这个预估的超平面也描述了我们模型中两个类别之间的最大间距。

比如说，一个在两个类别样本数据之间的预估的超平面可以用下面的图表来可视化地描述：

<center>
	<img src="/images/cljml/chap6/image7.png">
</center>

如上面的图标所示，圆圈和十字用来表示样本数据中两个不同类别的样本输入值。而直线就代表了支持向量机的预估超平面了。

在实践中，使用一个已经实现好的经过很多实际项目考验的支持向量机比我们自己实现一个支持向量机要高效和准确的多。现在已经有好多库中实现了支持向量机，并且都提供了多个语言的接口。其中一个库便是**LibLinear**(http://www.csie.ntu.edu.tw/~cjlin/liblinear/)，这个库中实现了使用支持向量机理论的现行分类器。`clj-liblinear`(https://github.com/lynaghk/clj-liblinear)这个库是Clojure对LibLinear库的一个封装，我们可以利用`clj-liblinear`这个库来很方便得使用Clojure建立一个利用支持向量机理论实现的线性分类器。

>要将clj-liblinear库加入到Leiningen项目中，只要在project.clj文件中加上这个库的依赖，如下所示：<br/>
[clj-liblinear "0.1.0"]<br/>
对于后面我们要实现的例子，需要修改我们项目文件中的名字空间声明，如下所示：<br/>
(ns my-namespace<br/>
&nbsp;&nbsp;(:use [clj-liblinear.core :only [train predict]]))

首先让我们来生成一些训练数据，这样我们就可以有属于两个类别的输入数据值了。在这一章的例子中我们将会对两类输入数据进行建模，生成训练数据代码如下所示：

{% codeblock lang:clojure %}
(def training-data
  (concat
   (repeatedly
    500 #(hash-map :class 0
                   :data {:x (rand)
                          :y (rand)}))
   (repeatedly
    500 #(hash-map :class 1
                   :data {:x (- (rand))
                          :y (- (rand))}))))
{% endcodeblock %}


