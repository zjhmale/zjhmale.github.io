---
layout: post
title: "cljmlchapter5-select data"
date: 2014-08-13 09:44:20 +0800
comments: true
categories: clojure ml
---

在之前的章节中，我们学习了**人工神经网络(ANNs)**以及如何用它们来有效地对非线性样本数据进行建模分析。至此我们已经讨论过好几个可以对给定训练数据集进行建模分析的机器学习技术了。在这一章中，我们将会从如何从样本数据中选择合适的特征的方面探讨以下几个议题：

* 我们会学习一些用于评估和量化指定出来的模型对给定的训练数据集建模的准确性。这些技术将会对于扩展或者调试一个已经训练好的模型非常有帮助。
* 我们会探索`clj-ml`这个库从而量化分析一个给定的机器学习模型。
* 到本章末尾时，我们会结合模型评估技术，实现一个垃圾邮件分类器。

**机器学习诊断**通常是用来描述一个测试过程，这个测试过程在执行时可以深入了解到一个机器学习内部什么在正常工作而什么不在正常工作。在诊断过程中获得的信息有助于我们提高给定机器学习模型的性能。通常情况下，在构建一个机器学习模型的过程中，最好可以并行地为这个模型指定一个诊断过程。构建一个模型诊断过程可能会花费和构建模型本身一样多的时间，但是非常值得去花这些时间去构建这个诊断过程，因为在它的帮助下可以快速的决定该如何对现有的模型进行修改和调整以获得更高的学习能力。因此从另一方面来说，构建一个诊断系统反而可以帮助我们节省调试和改进一个指定好的机器学习模型的时间。

另一个在机器学习领域中有趣的观点是说，假如我们不知道我们试图去建模拟合的数据的性质，我们就不能架设任何一种机器学习模型去拟合这些样本数据。这个公理也被称作**没有免费午餐**理论，可以总结如下：

>"假如无法得到关于一个学习算法性质的先验知识和，任何学习算法都不能说比其他的算法更好或者更烂(甚至是随机猜测)。"

## 理解欠拟合与过拟合

在之前的章节中，我们讨论了在制定一个机器学习模型的时候如何最小化所示函数的误差值。这的确是易于评估模型的总体误差变小，但是一个很小的误差通常来说并不能保证一个模型很好的拟合了给定的训练数据。所以在这一章中我们将会回顾和学习*过拟合*与*欠拟合*的概念。

对于一个预估模型来说，假如在预测时有很大的误差，那么就认为是欠拟合的情况。理想情况下，我们需要尽可能地减小模型的误差。然而，一个损失函数产生的误差很小的模型也未必可以准确地理解隐藏在给定特征之间的基本关系。而且，模型可能还会记住给定的训练数据集，这很有可能会导致模型也会对随机噪声进行建模和拟合。在过分学习了噪声的情况下，也被称作是过拟合了。一个过拟合模型最常见的症状就是可以很好地对已经学习过的样本数据做出准确的预测。但是在面对一个从来没有见到过的新的样本数据时却无法得到正确的结果。根据偏置-方差分解(_Bias-Variance Decomposition_)，假设我们有K个数据集，每个数据集都是从一个分布$$p(t,x)$$中独立的抽取出来的(t代表要预测的变量，x代表特征变量)。对于每个数据集D，我们都可以在其基础上根据学习算法来训练出一个模型$$y(x;D)$$来。在不同的数据集上进行训练可以得到不同的模型。学习算法的性能是根据在这K个数据集上训练得到的K个模型的平均性能来衡量的，亦即：

$$E_{D}[(y(x;D)-h(x))^{2}] = (E_{D}[y(x;D)]-h(x))^{2} + E_{D}[(y(x;D)-E_{D}[y(x;D)])^{2}]$$

其中第一项是偏差项，第二项是方差项，其中的x表示满足样本分布的随机变量。

所以可以看到一个欠拟合的模型存在**高偏差**，一个过拟合模型则具**高方差**。

假如我们要对单自变量和单因变量的数据集进行建模，那么理想情况下，这个模型不仅要能很好地拟合训练数据，对于没有在训练数据集中出现的新样本也要有很强的泛华能力。

在一个欠拟合模型中因变量随着自变量变化的趋势如下图所示：

<center>
	<img src="/images/cljml/chap5/image1.png">
</center>

在上图中，红色的叉表示的是训练数据集中得数据点。就像图中所示，一个欠拟合模型会存在较大的误差，所以我们需要选择合适的特征以及使用正则化技术来减小这个误差。

另一方面，假如一个模型总的误差值非常小的话也可能会出现过拟合的情况，从而使得预估模型没有办法对没有见过的数据进行准确的预测输出。一个过拟合模型的图像如下图所示：

<center>
	<img src="/images/cljml/chap5/image2.png">
</center>

如上图所示，预估模型为了的得到一个很小的总误差，从而过度学习了训练数据，从而对于新的数据没有办法走出正确地响应。

一个很好地拟合了训练样本数据的模型不但有比较小的总误差值，而且对于之前没有见到过的样本数据也能有很强的泛华能力。一个适当拟合的模型可以近似如下图所示：

<center>
	<img src="/images/cljml/chap5/image3.png">
</center>

人工神经网络对于给定的样本数据就可能出现欠拟合或者是过拟合的情况。比如一个神经网络隐含层的层数很少，并且隐含层中的节点也很少，那么就有可能会欠拟合，而如果一个神经网络中隐含层的层数太多或者隐含层中节点数过多就会出现过拟合。

## 评估模型

我们可以通过将因变量随自变量变化的趋势画出来的方法来判断模型是否过拟合或者欠拟合。但是当有大量的特征出现的时候，我们已经没有办法在二维图像中描绘这种趋势了，我们需要一种更好的可视化方法去判断一个模型对已有训练数据的拟合情况，以及对未知数据的泛华能力。

我们可以通过对不同的样布集来分别确定损失函数的值的方式来评估一个训练好的机器学习模型。因此我们需要将给定的数据切割成两份-一份用来做训练，而另一份用来做验证。后者的子集也被称作是模型的测试集。

然后利用$$N_{test}$$数量的样本作为测试集来计算模型损失度函数的值。这让我们可以用之前没有见到过的数据来衡量模型的总误差。这里用$$J_{test}(\hat{y})$$这一项表示评估模型$$\hat{y}$$用测试集计算出来的损失函数值，这一项也叫做这个训练后模型的**测试误差**。而在训练时产生的总误差叫做**训练误差**，并且用$$J_{train}(\hat{y})$$这一项表示。一个线性回归模型的测试误差可以用如下等式来计算：

$$J_{test}(\hat{y}) = \frac{1}{N_{test}}\sum_{i=1}^{N_{test}}(\hat{y}(X_{i})-Y_{i})^{2}$$

类似的，二分类模型中得测试误差也可以表达为如下形式：

$$J_{test}(\hat{y}) = \frac{1}{N_{test}}\sum_{i=1}^{N_{test}}err(\hat{y}(X_{i}), Y_{i}) \\
where \; err(\hat{y}(X_{i}), Y_{i}) = 1 \; if \; \hat{y}(X_{i}) \geq 0.5 \\
and \; err(\hat{y}(X_{i}), Y_{i}) = 0 \; if \; \hat{y}(X_{i}) < 0.5$$

确定模型的特征从而让测试误差减小的问题也叫做**模型选择**或者**特征选择**。当然为了避免过拟合，我们还必须要衡量模型在训练数据集之外的泛化能力。测试误差本身就是对模型在训练数据集之外的泛化误差的一种乐观估计。然而我们还是需要衡量模型在未见过数据上的泛化误差。假如这个模型在非训练集数据上也表现出很低的误差，我们可以基本断定模型没有对训练数据过拟合。这个过程叫做**交叉验证**。

因此，为了保证模型可以在没有见过的数据上也表现的很好，我们还需要一个额外的数据集，也被叫做**交叉验证集**。交叉验证集中样本的数量用$$N_{cv}$$这一项表示。典型情况下，样本数据需要被划分成训练集，测试集和交叉验证集，而且训练集中样本的数量要远大于测试集和交叉验证集。

泛化误差，或者说是交叉验证误差$$J_{cv}(\hat{y})$$确定了预估模型对未知数据拟合能力的性能。需要注意的是，在使用测试集和交叉测试集的时候，我们并没有去更新和修改模型本身。我们会在本章后面的部分深入的学习交叉验证，在后面的学习我们将会看到交叉验证是如何通过一些样本数据来决定一个模型的特征选择的。

举个例子，假如我们在训练数据集中有100个样本，我们需要将这些样本数据分成三个子集。前60个样本会被用来作为训练数据使得模型可以对数据很好的拟合。剩下的后面40个样本，其中20个会作为交叉验证集来评价模型，而最后的20个样本会作为测试集来测试经过交叉验证后的模型。

对于分类问题，展示一个分类器精度的很好的方法就是*混淆矩阵*。这种展示方法通常用来可视化一个基于监督学习算法分类器的分类性能。矩阵中的每一列代表某一类样本经过给定分类器中预测的结果，而每一行代表的是样本真正的类别。混淆矩阵也被称作训练完后的分类器的**应变矩阵**或者**误差矩阵**。

举例来讲解一下混淆矩阵，假如要用分类器做一个二分类，那么这个分类器的混淆矩阵会如下面所示：

<center>
	<img src="/images/cljml/chap5/image4.png">
</center>

在混淆矩阵中，预测出来的类别用竖直列来表示，而真实的列别使用横向行来表示。在上面的例子中，总共有100个样本，然而只有A类中的45个样本和B类中的10个样本被分类器正确分类了。A类中15个样本被分类到了B类，而B类中有30个样本被分类器分类到了A类，显然这是一个性能很差的分类器。

让我们来看另一个分类器的混淆矩阵，这个分类器使用了同样的样本数据，如下所示：

<center>
	<img src="/images/cljml/chap5/image5.png">
</center>

在上面这个混淆矩阵中，分类区对所有B类样本的分类预测结果都是正确地，并且仅仅只有5个A类的样本被错误地分到了B类。因此这个分类器模型相对于之前的那个更好地理解了两个类的特新与差别。所以在实际情况中，我们必须尽可能地让混淆矩阵中除了对角线元素以外的其他位置上元素的值都逼近于0。

## 理解特征选择

正如之前提到的，我们必须为我们的模型从样本数据中选取一套合适的特征。我们可以使用交叉验证机制来根据训练数据确定一组特征，会在下文中详细解释。

对于不同特征变量组合成的特征集合，我们都要用确定这个模型在使用某一组特征集时产生的训练误差和交叉验证误差。例如，我们可能会利用因变量构造高阶多项式作为新的特征。我们根据不同特征集中多项式的最高阶次为自变量，分别计算不同特征集的训练误差和交叉验证误差。我们可以画出这两个误差随着特征集中多项式最高次的不同变化的趋势图，如下所示：

<center>
	<img src="/images/cljml/chap5/image6.png">
</center>

根据上图所示，我们就可以根据$$J_{train}$$和$$J_{cv}$$的变化趋势来选取最合适的特征集。假如一个模型出入上图的左侧，那么这个模型有较高的训练误差和交叉验证误差，那么认为这个模型是对训练数据欠拟合的。另一方面，在上图的右侧的模型虽然训练误差非常小，但是交叉验证的误差很大，一般来说此时这个模型已经过拟合了。一般是选取两个误差都相对较小的时候对应的那一组特征集。

## 调整正则化系数

为了更好地拟合训练数据，我们可以使用正则化系数来避免过拟合问题。对于模型表现出来的行为，必须为给定模型选择一个合适的正则化系数值$$\lambda$$。可以注意到如果正则化系数值过高可能会导致过高的训练误差，这是我们不希望看到的。我们可以以正则化系数值为因变量，画出训练误差和交叉验证误差随之变化的曲线，如下所示：

<center>
	<img src="/images/cljml/chap5/image7.png">
</center>

如上图所示，我们可以通过修正正则化系数从而减小训练误差和交叉验证误差。假如一个模型两个误差都很高，那么我们就要考虑是否要减小正则化系数值直到两个误差对于给定的样本数据都有显著的减小量。

## 理解学习曲线

另一种有效衡量机器学习模型性能的方法是使用学习曲线。一个**学习曲线**本质上是描绘出了一个模型的误差随着对应的训练样本数量变化的趋势。例如，一个模型的训练误差和交叉验证误差的学习曲线可能会如下图所示：

<center>
	<img src="/images/cljml/chap5/image8.png">
</center>

学习曲线可以被用来诊断一个欠拟合或者是过拟合的模型。例如，随着训练样本数量的增加，我们可以观察到训练误差迅速增大并且最终收敛至靠近交叉验证误差值附近的位置。并且最终这个模型的两个误差值都很大。假如一个模型随训练样本数变化误差的变化情况像上面描述的那样，那么认为这个模型是欠拟合的，它的学习曲线可能会如下图所示：

<center>
	<img src="/images/cljml/chap5/image9.png">
</center>

另一方面，一个模型的训练误差随着训练样本数量的增加也可能增长得很缓慢，并且最终收敛到的位置的值和交叉验证误差值仍有很大的偏差，并没有收敛到交叉验证误差值附近。这样的模型就认为是过拟合了，其学习曲线可能如下图所示：

<center>
	<img src="/images/cljml/chap5/image10.png">
</center>

因此，学习曲线是进行交叉验证时一个很好的辅助工具，可以很好地确定机器学习模型中哪一部分没有正常工作，机器学习模型中哪一部分需要进行修改。

## 改进模型

一旦我们确定了一个模型对于给定的样本数据是欠拟合或者是过拟合，我们必须决定如何去改进这个模型以使得这个模型可以足够好的理解样本数据中自变量和因变量之间的关系。可以将改进方法做一个简单的介绍，如下所示：

* 增加或者去除特征。后面可以看到我们可以用这种发放来改进欠拟合或者过拟合模型。
* 修正正则化系数值$$\lambda$$。和第一种方式一样，这种方式也可以用来改进欠拟合或者过拟合模型。
* 收集更多的训练数据。这个方法对于改进过拟合模型是一种非常行之有效的方法，因为通过对更多的样本的学习，可以有效地提高模型的泛化能力。
* 根据模型中其他的特征构建高阶多项式从而为模型增加更多的特征。例如我们在对有两个因变量的数据进行建模，这两个特征表示为$$x_{1}$$和$$x_{2}$$，我们就可以构造出$$x_{1}^{2}$$，$$x_{2}^{2}$$和$$x_{1} \cdot x_{2}$$作为额外的特征输入模型从而改善模型的性能。多项式特征的阶次甚至可以更高一点，比如$$x_{1}^{3}$$和$$x_{1}^{2} \cdot x_{2}^{4}$$，但是这种方法可能因为引入更多的特征从而导致模型过于复杂而对于给定的训练数据又会产生过拟合的行为。

## 使用交叉验证

如我们之前简短介绍过的，交叉验证是一种常用的评估机器学习模型性能的验证技术。交叉验证本质上是在衡量一个预估模型对于训练之后给定的数据的泛化能力。这些数据不同于训练时传给模型的数据，这些训练之后传递给模型的数据被称作模型的**交叉验证集**，或者只是简单地称为**验证集**。对于个模型进行交叉验证，也叫做**轮转评估**或者**循环估计**。

假如一个预估模型在交叉验证中表现的很好，我们就可以认为这个模型能够很好的理解训练数据中因变量和自变量的内在关系。交叉验证的目标是对确定一个定制好后的模型是否对训练数据产生了过拟合的一种测试。从软件实现层面来讲，交叉验证可以说是机器学习系统的单元测试。

一轮交叉验证的过程中需要将所有可用的样本数据分成两个子集，然后用其中一个子集作为训练集，将另一个子集作为测试/验证集，或只是作为测试集，或者只是作为验证集。然后经过几轮这样的交叉验证的过程，每一次交叉验证都使用不同的数据集，最终要尽可能地减小给定模型交叉验证总误差值的方差值，也就是说最后要让所有交叉验证产生的总的误差值都差不多，没有太大的波动。如果要确定地衡量交叉误差产生的误差值，一种方式就是对所有交叉验证的结果求取均值。

我们可以实现很多种交叉验证机制来诊断给定的机器学习模型或者是系统。需要强调的是这些机制的简短介绍如下所述：

* 一种常见的交叉验证类型是**k折交叉验证(k-fold cross-validation)**，这种方法中，样本数据集被分割成*k*个子集，其中在一轮交叉验证中一个单独的子集被保留作为验证模型的数据，其他*k-1*个样本集作为训练集。交叉验证重复*k*次，也就是每一个子集都会作为验证集进行一轮交叉验证，然后平均这*k*次交叉验证产生的误差值结果或者使用其它的结合方式，最终得到一个单一的评估结果。这种方式的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10次交叉验证是最常见的。
* k折交叉验证的一种简单的变形是2折交叉验证，也就是k取2，也称作**holdout验证(holdout method)**。在2折交叉验证中，训练数据集和交叉验证集会有差不多相同的比例。
* **随机重复采样(Repeated random subsampling)**是另一个交叉验证的简单变形。在这个方式中首先从样本数据中随机组合选出一部分作为交叉验证数据，剩下的就当做训练数据。这种方法并不依赖于交叉验证的折叠次数，或者说是交叉验证进行的轮次。一般来说属于原样本数三分之一的数据被选作验证数据。
* 另一种*k折交叉验证*的变形是**留一(leave one out)**交叉验证。在这种方法中，每一次交叉验证时只是将一个单一样本来作为验证数据，而其他所有样本数据都作为训练数据。Leave-one-out交叉验证本质上是一个k折交叉验证方法，只不过这个k现在等于整一个样本数据集的大小，所以这种交叉验证的方法计算量比前几种都要大得多。

交叉验证只是简单地将预估模型当做一个黑盒，因为它并没对这个模型内部的结构做任何的假设。可以根据给定的样本，构造出好几组特征，然后利用交叉验证的方式去选择对样本数拟合最好的那一个特征集。当然，交叉验证并不是万能的，也会有一些限制，可以总结如下：

* 假如某一个特定的模型需要在内部进行特征选择，我们必须为每一次特征选择之后都对模型进行交叉验证，假如样本数量很大的话，计算成本将会非常昂贵。
* 只有当训练集和验证集是从相同的整体中抽取出来时，交叉验证才能得到有意义的结果。例如用某五年的股票市场的数据来训练一个股市预测模型，如果从后五年的数据来做交叉验证是没有意义的。另一个例子如果你需要预测某一种疾病发生趋势，而只是用某一些特定人群(*如青少年或者男性*)的数据来训练，而最后用所有人群的数据来做交叉验证，那么得到的总误差肯定是很大的。

总得来说，为我们建立的任何机器学习系统实现一个交叉验证机制都是很好的习惯。当然如何构建一个合适的交叉验证机制取决于我们试图建模的问题以及收集到的样本数据的性质。

>在下面的例子中，名字空间必须声明成如下形式：<br/>
(ns my-namespace<br/>
&nbsp;&nbsp;(:use [clj-ml classifier data]))

我们可以利用`clj-ml`库来为我们第三章建立的鱼产品包装厂分类器建立交叉验证。在那一章中我们使用`clj-ml`库来构建了一个确定一个鱼是三文鱼或者是鲈鱼的分类器。简要概述一下之前建立的分类器，一个鱼的样本被表示成一个向量，这个向量中存有鱼的类别以及各个特征的值。一条鱼的属性或者说是特征为长度，宽度以及表皮的亮度。我们也可以用一个膜拜来描述一个鱼的样本数据，如下所示：

{% codeblock lang:clojure %}
(def fish-template
     [{:category [:salmon :sea-bass]}
      :length :width :lightness])
{% endcodeblock %}

上面代码中定义的`fish-template`向量可以和一些样本数据一起来训练一个分类器模型。至此，我们先不去关心这个分类器用来对给定训练数据建模使用的分类算法是什么。我们仅仅只需要知道这个分类器使用`clj-ml`库中的`make-classifier`函数创建的，然后使用`*classifier*`这个变量来存储这个分类器对象，如下所示：

{% codeblock lang:clojure %}
(def *classifier* (make-classifier ...))
{% endcodeblock %}

注意上面的代码中我们需要用`*`包围住`classifier`，这在`lisp`的世界中俗称为耳罩(earmuffs)，`Clojure`中如果要声明一个动态变量就需要加上这个耳罩，这没有任何语法含义，只是一种约定，方便其他的人阅读修改你的`lisp`代码。

假设已经使用了一些样本数据训练好了一个分类器。我们必须去评估这个已经训练好的分类器模型。为了做到这一点，我们必须首先创建一些样本数据用于交叉验证。为了简单起见，我们在这个例子中选择随机产生的数据作为验证。我们可以使用第三章中已经定义过的`make-sample-fish`函数来产生验证数据。
